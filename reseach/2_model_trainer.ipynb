{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db20692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/leksman/Desktop/EEEEE/end_to_end_plant_vilage_project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309fa90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "   dir_root: Path\n",
    "   train_data_root: Path\n",
    "   trained_model: Path\n",
    "   num_epoch: int\n",
    "   learning_rate: float\n",
    "   num_classes: int\n",
    "   batch_size: int\n",
    "   num_workers: int\n",
    "   shuffle: bool\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea8989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the config \n",
    "from scr.Plant_Vilage.constants import SCHEMA_FILE_PATH,PARAMS_FILE_PATH,CONFIG_FILE_PATH\n",
    "import scr.Plant_Vilage.utils.common as common\n",
    "create_diretory = common.create_directory \n",
    "read_yaml = common.read_yaml\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_file_path: Path = CONFIG_FILE_PATH,\n",
    "        schema_file_path: Path = SCHEMA_FILE_PATH,\n",
    "        params_file_path: Path = PARAMS_FILE_PATH\n",
    "    ):\n",
    "       \n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.schema = read_yaml(schema_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        create_diretory([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.model_params\n",
    "        create_diretory([config.dir_root])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            dir_root= config.dir_root,\n",
    "            train_data_root=config.train_data_root,\n",
    "            trained_model=config.trained_model,\n",
    "            num_epoch = params.num_epoch,\n",
    "            learning_rate=params.learning_rate,\n",
    "            num_classes=params.num_classes,\n",
    "            batch_size=params.batch_size,\n",
    "            num_workers=params.num_workers,\n",
    "            shuffle=params.shuffle\n",
    "    \n",
    "\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76770a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "import gc\n",
    "from scr.Plant_Vilage import logger\n",
    "from scr.Plant_Vilage.utils.common import save_bin\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self,config:ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.device =  torch.device(\"cuda\")\n",
    "\n",
    "    def train_cnn_model(self):\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),                     # Resize images\n",
    "        transforms.RandomHorizontalFlip(p=0.5),            # Randomly flip images\n",
    "        transforms.RandomRotation(degrees=15),             # Random rotation\n",
    "        #transforms.ColorJitter(brightness=0.2),  # Add jitter\n",
    "        transforms.ToTensor(),                             # Convert to tensor [C, H, W] in [0, 1]\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],          # Normalize with 0â€“1 range scaling\n",
    "                            std=[0.5, 0.5, 0.5])\n",
    "                ])\n",
    "\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(root=self.config.train_data_root,transform=transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=self.config.shuffle, num_workers=self.config.num_workers)\n",
    "\n",
    "        class BasicBlock(nn.Module):\n",
    "            def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "                super().__init__()\n",
    "                self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                                    stride=stride, padding=1, bias=False)\n",
    "                self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "                self.relu = nn.ReLU(inplace=True)\n",
    "                self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                                    stride=1, padding=1, bias=False)\n",
    "                self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "                self.downsample = downsample\n",
    "\n",
    "            def forward(self, x):\n",
    "                identity = x\n",
    "\n",
    "                out = self.relu(self.bn1(self.conv1(x)))\n",
    "                out = self.bn2(self.conv2(out))\n",
    "\n",
    "                if self.downsample:\n",
    "                    identity = self.downsample(x)\n",
    "\n",
    "                out += identity\n",
    "                out = self.relu(out)\n",
    "\n",
    "                return out\n",
    "\n",
    "        class ResNetLike(nn.Module):\n",
    "            def __init__(self, num_classes=1):\n",
    "                super().__init__()\n",
    "                self.in_channels = 64\n",
    "                self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "                self.bn1 = nn.BatchNorm2d(64)\n",
    "                self.relu = nn.ReLU(inplace=True)\n",
    "                self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "                self.layer1 = self._make_layer(64, 2)\n",
    "                self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "                self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "                self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "                self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "                self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "            def _make_layer(self, out_channels, blocks, stride=1):\n",
    "                downsample = None\n",
    "                if stride != 1 or self.in_channels != out_channels:\n",
    "                    downsample = nn.Sequential(\n",
    "                        nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                    )\n",
    "\n",
    "                layers = [BasicBlock(self.in_channels, out_channels, stride, downsample)]\n",
    "                self.in_channels = out_channels\n",
    "                for _ in range(1, blocks):\n",
    "                    layers.append(BasicBlock(out_channels, out_channels))\n",
    "\n",
    "                return nn.Sequential(*layers)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.relu(self.bn1(self.conv1(x)))  # [B, 64, H/2, W/2]\n",
    "                x = self.pool(x)                        # [B, 64, H/4, W/4]\n",
    "                x = self.layer1(x)                      # -> [B, 64, H/4, W/4]\n",
    "                x = self.layer2(x)                      # -> [B, 128, H/8, W/8]\n",
    "                x = self.layer3(x)                      # -> [B, 256, H/16, W/16]\n",
    "                x = self.layer4(x)                      # -> [B, 512, H/32, W/32]\n",
    "                x = self.global_pool(x)                 # -> [B, 512, 1, 1]\n",
    "                x = torch.flatten(x, 1)                 # -> [B, 512]\n",
    "                x = self.fc(x)                          # -> [B, num_classes]\n",
    "                return x\n",
    "            \n",
    "\n",
    "\n",
    "        def get_model_optimizer():\n",
    "            net = ResNetLike(num_classes=38)              # Make sure num_classes matches your dataset\n",
    "            lossFun = nn.CrossEntropyLoss()               # For multi-class classification\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=self.config.learning_rate)\n",
    "            return net, optimizer, lossFun\n",
    "        \n",
    "\n",
    "        def train_model(x_y_train_loader, device):\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            net, optimizer, lossFun = get_model_optimizer()\n",
    "            net.to(device)\n",
    "\n",
    "            num_epoch = self.config.num_epoch\n",
    "            train_acc = np.zeros(num_epoch)\n",
    "            train_loss = np.zeros(num_epoch)\n",
    "            accumulation_step = 16\n",
    "\n",
    "            for epoch in range(num_epoch):\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss = []\n",
    "                batch_acc = []\n",
    "\n",
    "                for i, (X, y) in enumerate(x_y_train_loader):\n",
    "                    # Flatten image input for FNN\n",
    "                    X = X.to(device)  # (batch_size, 3*224*224)\n",
    "                    y = y.to(device).long()  # CrossEntropyLoss expects LongTensor class indices\n",
    "\n",
    "                    y_pred = net(X)\n",
    "                    pred_labels = y_pred.argmax(dim=1)     # Get predicted class indices\n",
    "                    acc = (pred_labels == y).float().mean().item()\n",
    "                    batch_acc.append(acc)\n",
    "\n",
    "                    loss = lossFun(y_pred, y)\n",
    "                    loss = loss / accumulation_step\n",
    "                    batch_loss.append(loss.item() * accumulation_step)  # Undo scaling for logging\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    if (i + 1) % accumulation_step == 0 or (i + 1) == len(x_y_train_loader):\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                train_acc[epoch] = np.mean(batch_acc)\n",
    "                train_loss[epoch] = np.mean(batch_loss)\n",
    "\n",
    "                \n",
    "\n",
    "                logger.info(f\"Epoch {epoch+1}/{num_epoch} | \"\n",
    "                    f\"Train Loss: {train_loss[epoch]:.4f}, Train Acc: {train_acc[epoch]:.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            torch.save(net.state_dict(),self.config.trained_model)\n",
    "            logger.info(f\"Plant Vilage Modeled SUcessfully the trained model is located at {self.config.trained_model}\")\n",
    "\n",
    "\n",
    "        train_model(x_y_train_loader=train_loader,device=self.device)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9920fc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-16 00:32:48,187: INFO: common: YAML file loaded successfully from: config_yaml/config.yaml:]\n",
      "[2025-09-16 00:32:48,193: INFO: common: YAML file loaded successfully from: schema.yaml:]\n",
      "[2025-09-16 00:32:48,196: INFO: common: YAML file loaded successfully from: params.yaml:]\n",
      "[2025-09-16 00:32:48,197: INFO: common: Created directory at: artifacts:]\n",
      "[2025-09-16 00:32:48,198: INFO: common: Created directory at: artifacts/model_trainer:]\n",
      "[2025-09-16 00:35:22,092: INFO: 2071250403: Epoch 1/10 | Train Loss: 1.6693, Train Acc: 0.5273:]\n",
      "[2025-09-16 00:37:53,173: INFO: 2071250403: Epoch 2/10 | Train Loss: 0.6786, Train Acc: 0.7912:]\n",
      "[2025-09-16 00:40:25,791: INFO: 2071250403: Epoch 3/10 | Train Loss: 0.3969, Train Acc: 0.8746:]\n",
      "[2025-09-16 00:42:57,944: INFO: 2071250403: Epoch 4/10 | Train Loss: 0.2802, Train Acc: 0.9095:]\n",
      "[2025-09-16 00:45:29,667: INFO: 2071250403: Epoch 5/10 | Train Loss: 0.2057, Train Acc: 0.9331:]\n",
      "[2025-09-16 00:48:01,892: INFO: 2071250403: Epoch 6/10 | Train Loss: 0.1728, Train Acc: 0.9437:]\n",
      "[2025-09-16 00:50:34,090: INFO: 2071250403: Epoch 7/10 | Train Loss: 0.1412, Train Acc: 0.9555:]\n",
      "[2025-09-16 00:53:06,237: INFO: 2071250403: Epoch 8/10 | Train Loss: 0.1211, Train Acc: 0.9617:]\n",
      "[2025-09-16 00:55:38,064: INFO: 2071250403: Epoch 9/10 | Train Loss: 0.1063, Train Acc: 0.9651:]\n",
      "[2025-09-16 00:58:10,262: INFO: 2071250403: Epoch 10/10 | Train Loss: 0.0888, Train Acc: 0.9707:]\n",
      "[2025-09-16 00:58:10,597: INFO: 2071250403: Plant Vilage Modeled SUcessfully the trained model is located at artifacts/model_trainer/cnn_model.pth:]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    train_cnn_model = ModelTrainer(config=model_trainer_config)\n",
    "    train_cnn_model.train_cnn_model()\n",
    "except Exception as e:\n",
    "    raise e    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
